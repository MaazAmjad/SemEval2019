{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2019 Hyperpartisan News Detection\n",
    "#### Make a baseline system that uses only the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 645 samples, of which half would be used for training, and half for testing\n",
    "\n",
    "What can we try with ~300 sample?\n",
    "1. feature dimension shouldn't really go beyond 300\n",
    "    - need dense and compact representation wod2vec/glove\n",
    "2. complexity of classifier shouldn't be too high\n",
    "    - linear SVM\n",
    "    - LR\n",
    "    - ensemble\n",
    "3. pre-trained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.etree import iterparse\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthHandler(xml.sax.ContentHandler):\n",
    "    def __init__(self, gt):\n",
    "        xml.sax.ContentHandler.__init__(self)\n",
    "        self.gt = gt\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == \"article\":\n",
    "            articleId = attrs.getValue(\"id\")\n",
    "            self.gt.append(attrs.getValue(\"hyperpartisan\"))\n",
    "            \n",
    "def readFiles(textFile, labelFile):\n",
    "    X,y = [], []\n",
    "    \n",
    "    with open(labelFile) as labelFile:\n",
    "        xml.sax.parse(labelFile, GroundTruthHandler(y))\n",
    "       \n",
    "    for event, elem in iterparse(textFile):\n",
    "        if elem.tag == \"article\":\n",
    "            title = elem.attrib['title']\n",
    "            text = \"\".join(elem.itertext())\n",
    "            title = cleanQuotations(title)\n",
    "            text = cleanQuotations(text)\n",
    "            text = cleanText(fixup(text))\n",
    "            text = ' '.join(text.split()[:1000])\n",
    "            X.append(title + \". \" + text)\n",
    "            elem.clear()\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in labels and texts\n",
    "textFile = '../data/articles-training-byarticle.xml'\n",
    "labelFile = \"../data/ground-truth-training-byarticle.xml\"\n",
    "texts, labels = readFiles(textFile, labelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id1, id2 = fixedTestSplit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gloveVectorize(glove, text):\n",
    "    dim = len(glove[\"the\"])\n",
    "    X = np.zeros( (len(text), dim) )\n",
    "    for text_id, t in enumerate(text):\n",
    "        tmp = np.zeros((1,300))\n",
    "        words = customTokenize(t)\n",
    "        words = [w for w in words if w in glove.keys()]\n",
    "        for word in words:\n",
    "            tmp[:] += glove[word]\n",
    "            X[text_id, :] = tmp/len(words)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_texts = gloveVectorize(read_glove(300), texts)\n",
    "glove_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = glove_texts[id1]\n",
    "test_x = glove_texts[id2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear] C=0.005000 P=l1 | acc=0.645659\n",
      "[Linear] C=0.005000 P=l2 | acc=0.683171\n",
      "[Linear] C=0.010000 P=l1 | acc=0.736504\n",
      "[Linear] C=0.010000 P=l2 | acc=0.683071\n",
      "[Linear] C=0.100000 P=l1 | acc=0.711309\n",
      "[Linear] C=0.100000 P=l2 | acc=0.645653\n",
      "[Linear] C=0.500000 P=l1 | acc=0.670280\n",
      "[Linear] C=0.500000 P=l2 | acc=0.642528\n",
      "[Linear] C=1.000000 P=l1 | acc=0.651726\n",
      "[Linear] C=1.000000 P=l2 | acc=0.642528\n"
     ]
    }
   ],
   "source": [
    "C = [0.01, 0.1, 1, 10]\n",
    "for c in C:\n",
    "    for p in ['l1', 'l2']:\n",
    "        svm = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", LinearSVC(C=c, penalty=p, dual=False, tol=1e-4, max_iter=10000))\n",
    "        ])\n",
    "        print(\"[Linear] C=%f P=%s | acc=%f\" %(c,p,np.mean(cross_val_score(svm, train_x, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kernel] C=0.700000 | acc=0.770506\n",
      "[Kernel] C=0.800000 | acc=0.776662\n",
      "[Kernel] C=0.900000 | acc=0.776662\n",
      "[Kernel] C=1.000000 | acc=0.773442\n",
      "[Kernel] C=1.100000 | acc=0.779597\n",
      "[Kernel] C=2.000000 | acc=0.770122\n",
      "[Kernel] C=3.000000 | acc=0.769837\n"
     ]
    }
   ],
   "source": [
    "C = [0.7, 0.8, 0.9, 1, 1.1, 2, 3]\n",
    "for c in C:\n",
    "    kernel_svm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(C=c, gamma=\"auto\", max_iter = 5000))\n",
    "    ])\n",
    "    print(\"[Kernel] C=%f | acc=%f\" %(c,np.mean(cross_val_score(kernel_svm, train_x, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.010000 | acc=0.630425\n",
      "C=0.100000 | acc=0.693038\n",
      "C=1.000000 | acc=0.764061\n",
      "C=10.000000 | acc=0.717464\n",
      "C=100.000000 | acc=0.680236\n"
     ]
    }
   ],
   "source": [
    "# we test LR with differen regularization params and 2 different glove dimensions\n",
    "C = [0.01, 0.1, 1, 10, 100]\n",
    "for c in C:\n",
    "    lr_clf = Pipeline([\n",
    "            #(\"scaler\", StandardScaler()),\n",
    "            #(\"pca\", PCA(n_components=0.95)),\n",
    "            (\"lr\", LogisticRegression(solver = 'lbfgs', C = c, max_iter=1000))\n",
    "    ])\n",
    "    print(\"C=%f | acc=%f\" %(c,np.mean(cross_val_score(lr_clf, train_x, labels[id1], cv=10))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best accuracy: 77.96 with SVM, C=1.1, glove=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7894736842105263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[179,  25],\n",
       "       [ 43,  76]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=1.1, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "\n",
    "model.fit(train_x, labels[id1])\n",
    "tst_pred = model.predict(test_x)\n",
    "print('Test accuracy: ', accuracy_score(labels[id2], tst_pred))\n",
    "confusion_matrix(labels[id2], tst_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to all samples\n",
    "model.fit(glove_texts, labels)\n",
    "# save the model\n",
    "pickle.dump(model, open('./svm_glove.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
